{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "from icecream import ic\n",
    "from data_loader import *\n",
    "from FFNN_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "torch.Size([4, 25])\n",
      "cpu\n",
      "Shape of input tensor: torch.Size([64, 4, 25])\n",
      "Shape of labels tensor: torch.Size([64, 4, 1])\n",
      "Shape of input tensor in MultiIonReadout: torch.Size([64, 4, 25])\n",
      "Shape of output tensor in MultiIonReadout: torch.Size([64, 4, 1])\n",
      "Shape of input tensor 'y' in MultiIonReadout -> Encoder and Shared_Encoder: torch.Size([64, 100])\n",
      "Shape of input 'X' tensor in IndexDependentDense: torch.Size([64, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (2) for operand 1 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of input tensor:\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# PRINT\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of labels tensor:\u001b[39m\u001b[38;5;124m\"\u001b[39m, labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 72\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbceloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Seiven\\Desktop\\UW\\MY_MLmodels\\ION_Classifier\\FFNN_model.py:91\u001b[0m, in \u001b[0;36mMultiIonReadout.bceloss\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of input tensor in MultiIonReadout:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of output tensor in MultiIonReadout:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, y)\n",
      "File \u001b[1;32mc:\\Users\\Seiven\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Seiven\\Desktop\\UW\\MY_MLmodels\\ION_Classifier\\FFNN_model.py:81\u001b[0m, in \u001b[0;36mMultiIonReadout.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     79\u001b[0m y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m#Reshaping the data for the forwards pass\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of input tensor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in MultiIonReadout -> Encoder and Shared_Encoder:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 81\u001b[0m y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_encoder(y)\n\u001b[0;32m     83\u001b[0m y_concat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([y1, y2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#Combining the data for each y node for returning the classification\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seiven\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Seiven\\Desktop\\UW\\MY_MLmodels\\ION_Classifier\\FFNN_model.py:44\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Seiven\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Seiven\\Desktop\\UW\\MY_MLmodels\\ION_Classifier\\FFNN_model.py:29\u001b[0m, in \u001b[0;36mIndexDependentDense.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN_i)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of input \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m tensor in IndexDependentDense:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 29\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnij,bni->bnj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# \"...\" removed and added just b\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of output \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m tensor in IndexDependentDense:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Seiven\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    380\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (2) for operand 1 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "class Labelled_Dataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file = h5py.File(file_path, \"r\")\n",
    "        self.measurements = self.file[\"measurements\"]\n",
    "        self.labels = self.file[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.measurements.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.measurements[idx])  # , dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx])  # , dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "file_path = \"binary/reformatted_dataset.h5\"\n",
    "\n",
    "dataset = Labelled_Dataset(file_path)\n",
    "print(len(dataset))  # PRINT\n",
    "print(dataset[0][0].shape)\n",
    "batch_size = 64\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(str(device))\n",
    "\n",
    "N = 4  # number of images in whole picture\n",
    "N_i = 25\n",
    "N_h = 256\n",
    "N_o = 1\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# model\n",
    "encoder = Encoder(N, N_i, N_h)\n",
    "shared_encoder = SharedEncoder(N_i, N_h)\n",
    "classifier = Classifier(N, N_h * 2, N_o)\n",
    "model = MultiIonReadout(encoder, shared_encoder, classifier)\n",
    "\n",
    "# Setting the hyperparameters\n",
    "N_epochs = 50\n",
    "lr = 1e-3\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "schedule_params = {\"factor\": 1}\n",
    "schedule = lr_scheduler.ConstantLR(optimizer, **schedule_params)\n",
    "log_every = 1\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_epochs):\n",
    "\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Shape of input tensor:\", inputs.shape)  # PRINT\n",
    "        print(\"Shape of labels tensor:\", labels.shape)\n",
    "        loss = model.bceloss(inputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation loop\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            loss = model.bceloss(inputs, labels)\n",
    "            accuracy = model.accuracy(inputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        avg_accuracy = total_accuracy / len(val_loader)\n",
    "\n",
    "    print(\n",
    "        \"\\r Epoch {}/{}, Training Loss = {}, Val Loss = {}, Val Acc = {}\".format(\n",
    "            epoch + 1, N_epochs, loss.item(), avg_loss, avg_accuracy\n",
    "        ),\n",
    "        end=\"\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
