{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process of MSE implementing \n",
    "\n",
    "our currect distribution sample: \n",
    "\n",
    "ic| normalized_counts: [0.057,\n",
    "                        0.068,\n",
    "                        0.051,\n",
    "                        0.072,\n",
    "                        0.071,\n",
    "                        0.095,\n",
    "                        0.043,\n",
    "                        0.080,\n",
    "                        0.049,\n",
    "                        0.062,\n",
    "                        0.066,\n",
    "                        0.061,\n",
    "                        0.070,\n",
    "                        0.059,\n",
    "                        0.053,\n",
    "                        0.043]\n",
    "\n",
    "uniform_count: 0.0625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function design problem. \n",
    "\n",
    "Background:\n",
    "    We need to include unlabelled data into our model to maximize accuracy of predicting 16 classes of our system. To acheieve this goal we pretrained simple Encoder-Decoder model on our labelled data (91-92% accuracy). Now, our job is set up a programm which will use pretrain network for inicial classificationo with following LOSS which will adjust NEW model to classify unlabelled data more accuratly. \n",
    "\n",
    "    So far we tried multiple different ways to make it work. MSE loss, CrossEntropy, addition of gumble softmax function, and finally Correlation loss with multiple modification and changes. \n",
    "\n",
    "\n",
    "Loss isn't working in any of the cases. Originally I had problems with discritness of Counter function needed for MSE (we can't use threshold method or counter). Gumble sofmax with correlation loss fixed the problem with discritness and step function, but loss didn't allow the network to learn all the states, we arrived to binary scenario, where we either learn that all states are 0000 or 1111. \n",
    "\n",
    "Problem statement:\n",
    "\n",
    "What are we doing is, we pass unlabelled data to our pretrained model and receive predictions. We want to construct a disctribution of 16 unique states we have predicted and construct loss to push it as close as possible to uniform. \n",
    "\n",
    "in code we have \"samples\" as an input from our pre-saved model classification. we need to pass these predictions to the loss function.\n",
    "\n",
    "Input new_model receives (first 10 samples) from an old pretrained:\n",
    "\n",
    "tensor([[[0.5724], \n",
    "         [0.1302], \n",
    "         [0.2840], \n",
    "         [0.7402]],\n",
    "\n",
    "        [[0.9580], \n",
    "         [0.9998], \n",
    "         [0.9998], \n",
    "         [1.0000]],\n",
    "\n",
    "        [[0.9959], \n",
    "         [0.3668], \n",
    "         [0.9714], \n",
    "         [0.1976]],\n",
    "\n",
    "        [[0.9451], \n",
    "         [0.9999], \n",
    "         [0.2559], \n",
    "         [0.1247]],\n",
    "\n",
    "        [[0.5298], \n",
    "         [0.1069], \n",
    "         [0.4578],\n",
    "         [0.0506]],\n",
    "\n",
    "        [[0.9986],\n",
    "         [0.0427],\n",
    "         [0.1707],\n",
    "         [0.0350]],\n",
    "\n",
    "        [[0.9998],\n",
    "         [0.7080],\n",
    "         [0.9746],\n",
    "         [0.9995]],\n",
    "\n",
    "        [[0.0520],\n",
    "         [0.0258],\n",
    "         [0.4781],\n",
    "         [0.0730]],\n",
    "\n",
    "        [[0.0538],\n",
    "         [0.1909],\n",
    "         [0.3127],\n",
    "         [0.1140]],\n",
    "\n",
    "        [[0.3626],\n",
    "         [0.9971],\n",
    "         [0.1013],\n",
    "         [0.7093]]], grad_fn=<SliceBackward0>)\n",
    "\n",
    "We used F.gumbel_softmax to avoid step function (which is discrete) and get this output before passing it to our loss. But this approach could be false. \n",
    "\n",
    "\n",
    "\n",
    "Gumbel samples: \n",
    "tensor([[[1., 0.],\n",
    "         [1., 0.],\n",
    "         [1., 0.],\n",
    "         [0., 1.]],\n",
    "\n",
    "        [[0., 1.],\n",
    "         [0., 1.],\n",
    "         [0., 1.],\n",
    "         [0., 1.]],\n",
    "\n",
    "        [[0., 1.],\n",
    "         [1., 0.],\n",
    "         [0., 1.],\n",
    "         [1., 0.]],\n",
    "\n",
    "        [[0., 1.],\n",
    "         [0., 1.],\n",
    "         [1., 0.],\n",
    "         [1., 0.]],\n",
    "\n",
    "        [[0., 1.],\n",
    "         [1., 0.],\n",
    "         [0., 1.],\n",
    "         [1., 0.]],\n",
    "\n",
    "        [[0., 1.],\n",
    "         [0., 1.],\n",
    "         [1., 0.],\n",
    "         [1., 0.]],\n",
    "\n",
    "        [[0., 1.],\n",
    "         [0., 1.],\n",
    "         [0., 1.],\n",
    "         [0., 1.]],\n",
    "\n",
    "        [[1., 0.],\n",
    "         [1., 0.],\n",
    "         [1., 0.],\n",
    "         [1., 0.]],\n",
    "\n",
    "        [[1., 0.],\n",
    "         [1., 0.],\n",
    "         [1., 0.],\n",
    "         [1., 0.]],\n",
    "\n",
    "        [[1., 0.],\n",
    "         [0., 1.],\n",
    "         [1., 0.],\n",
    "         [1., 0.]]], grad_fn=<SliceBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mN_epochs\u001b[49m):\n\u001b[0;32m      3\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, xbatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(halfpi_train_loader):  \u001b[38;5;66;03m# Adjusted for unlabelled data\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'N_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(N_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, xbatch in enumerate(halfpi_train_loader):  # Adjusted for unlabelled data\n",
    "        optimizer.zero_grad()\n",
    "        xbatch = xbatch.to(device=device)\n",
    "\n",
    "        # Ensure the input tensor has requires_grad=True\n",
    "        xbatch.requires_grad_(True)\n",
    "\n",
    "        batch_loss = corrloss(xbatch)\n",
    "        batch_loss.backward()\n",
    "\n",
    "        running_loss += (\n",
    "            batch_loss.item()\n",
    "        )  # Using .item() to avoid accumulation of the graph\n",
    "\n",
    "        optimizer.step()\n",
    "    step += 1\n",
    "\n",
    "    running_loss /= i + 1\n",
    "\n",
    "    schedule.step()\n",
    "\n",
    "    if epoch % log_every == 0 or epoch == N_epochs - 1:\n",
    "        val_loss = torch.tensor(\n",
    "            [\n",
    "                corrloss(\n",
    "                    databatch.to(device=device)\n",
    "                )  # Call the corrloss function directly\n",
    "                for databatch in halfpi_val_loader\n",
    "            ]\n",
    "        ).mean()\n",
    "\n",
    "        print(\n",
    "            \"{:<100}\".format(\n",
    "                \"\\r\"\n",
    "                + \"[{:<60}] \".format(\n",
    "                    \"=\" * ((np.floor((epoch + 1) / N_epochs * 60)).astype(int) - 1)\n",
    "                    + \">\"\n",
    "                    if epoch + 1 < N_epochs\n",
    "                    else \"=\" * 60\n",
    "                )\n",
    "                + \"{:<40}\".format(\n",
    "                    \"Epoch {}/{}: Loss(Train) = {:.3f}, Loss(Val) = {:.3f}\".format(\n",
    "                        epoch + 1, N_epochs, running_loss, val_loss\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            end=\"\",\n",
    "        )\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_counts(predictions, num_states=16):\n",
    "\n",
    "    binary_preds = (predictions > 0.5).int()\n",
    "    binary_strings = [\n",
    "        \"\".join(map(str, preds)) for preds in binary_preds.cpu().numpy().reshape(-1, 4)\n",
    "    ]\n",
    "    tensor_str_counts = Counter(binary_strings)\n",
    "    # counts to probs\n",
    "    counts = [tensor_str_counts.get(f\"{i:04b}\", 0) for i in range(num_states)]\n",
    "    probabilities = torch.tensor(counts, dtype=torch.float32) / len(binary_strings)\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def kldiv_loss(predictions, num_classes):\n",
    "    predicted_probs = get_state_counts(predictions, num_classes)\n",
    "\n",
    "    uniform_probs = torch.full_like(predicted_probs, 1.0 / num_classes)\n",
    "\n",
    "    # Ensure probs are non-zero\n",
    "    predicted_probs = torch.clamp(predicted_probs, min=1e-10)\n",
    "    uniform_probs = torch.clamp(uniform_probs, min=1e-10)\n",
    "\n",
    "    # Compute KL\n",
    "    kl_div = F.kl_div(predicted_probs.log(), uniform_probs, reduction=\"batchmean\")\n",
    "\n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper parameters optimization block: \n",
    "log_every = 1\n",
    "\n",
    "########################################################################################\n",
    "# Best hyperparameters:  {'lr': 0.006185526944778624, 'weight_decay': 0.009880060840556128}\n",
    "\n",
    "def objective(trial):\n",
    "    N_epochs = 50\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1)\n",
    "\n",
    "    model = MultiIonReadout(encoder, classifier).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    schedule_params = {\"factor\": 1}\n",
    "    schedule = lr_scheduler.ConstantLR(optimizer, **schedule_params)\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(N_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, databatch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            (xbatch, ybatch) = databatch\n",
    "            xbatch = xbatch.to(device=device)\n",
    "            ybatch = ybatch.to(device=device)\n",
    "\n",
    "            batch_loss = model.train().nllloss(xbatch, ybatch)\n",
    "            batch_loss.backward()\n",
    "\n",
    "            running_loss += batch_loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        running_loss /= i + 1\n",
    "\n",
    "        schedule.step()\n",
    "\n",
    "        if epoch % log_every == 0 or epoch == N_epochs - 1:\n",
    "            val_loss = (\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        model.eval().nllloss(\n",
    "                            databatch[0].to(device=device),\n",
    "                            databatch[1].to(device=device),\n",
    "                        )\n",
    "                        for databatch in val_loader\n",
    "                    ]\n",
    "                )\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "\n",
    "            sia = (\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        model.eval().sia(\n",
    "                            databatch[0].to(device=device),\n",
    "                            databatch[1].to(device=device),\n",
    "                        )\n",
    "                        for databatch in val_loader\n",
    "                    ]\n",
    "                )\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"{:<180}\".format(\n",
    "                    \"\\r\"\n",
    "                    + \"[{:<60}] \".format(\n",
    "                        \"=\" * ((np.floor((epoch + 1) / N_epochs * 60)).astype(int) - 1)\n",
    "                        + \">\"\n",
    "                        if epoch + 1 < N_epochs\n",
    "                        else \"=\" * 60\n",
    "                    )\n",
    "                    + \"{:<40}\".format(\n",
    "                        \"Epoch {}/{}: NLL Loss(Train) = {:.3g}, NLL Loss(Val) = {:.3g}, Accuracy(Val) = {:.3f}\".format(\n",
    "                            epoch + 1, N_epochs, running_loss, val_loss, sia\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "                end=\"\",\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# Creating a study object and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best trial: {study.best_trial.value}\")\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters:  {'lr': 0.00016388181712790806, 'weight_decay': 4.6547937254492916e-05}\n",
    "def objective(trial):\n",
    "    N_epochs = 20\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1)\n",
    "\n",
    "    optimizer = Adam(enhanced_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    schedule_params = {\"factor\": 1}\n",
    "    schedule = lr_scheduler.ConstantLR(optimizer, **schedule_params)\n",
    "    log_every = 1\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(N_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, xbatch in enumerate(halfpi_train_loader):  # Correct unpacking\n",
    "            optimizer.zero_grad()\n",
    "            xbatch = xbatch.to(device=device)\n",
    "            xbatch.requires_grad_(True)\n",
    "\n",
    "            batch_loss = enhanced_model.train().corrloss(xbatch)\n",
    "            batch_loss.backward()\n",
    "\n",
    "            if batch_loss.isnan():\n",
    "                break\n",
    "\n",
    "            running_loss += batch_loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        running_loss /= i + 1\n",
    "\n",
    "        schedule.step()\n",
    "\n",
    "        if epoch % log_every == 0 or epoch == N_epochs - 1:\n",
    "            val_loss = (\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        enhanced_model.eval().corrloss(databatch.to(device=device))\n",
    "                        for databatch in halfpi_val_loader\n",
    "                    ]\n",
    "                )\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "\n",
    "            sia = (\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        enhanced_model.eval().sia(\n",
    "                            databatch[0].to(device=device),\n",
    "                            databatch[1].to(device=device),\n",
    "                        )\n",
    "                        for databatch in val_loader\n",
    "                    ]\n",
    "                )\n",
    "                .mean()\n",
    "                .item()\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"{:<180}\".format(\n",
    "                    \"\\r\"\n",
    "                    + \"[{:<60}] \".format(\n",
    "                        \"=\" * ((np.floor((epoch + 1) / N_epochs * 60)).astype(int) - 1)\n",
    "                        + \">\"\n",
    "                        if epoch + 1 < N_epochs\n",
    "                        else \"=\" * 60\n",
    "                    )\n",
    "                    + \"{:<40}\".format(\n",
    "                        \"Epoch {}/{}: Correlation Loss(Train) = {:.3f}, Correlation Loss(Val) = {:.3f}, Accuracy(All Bright & Dark) = {:.3f}\".format(\n",
    "                            epoch + 1, N_epochs, running_loss, val_loss, sia\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "                end=\"\",\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best trial: {study.best_trial.value}\")\n",
    "print(\"Best hyperparameters: \", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (731669184.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    Label 0: [[0.]\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## new structure of our dataset:\n",
    "\n",
    "Label 0: [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "Measurement 0: [[201. 198. 202. 201. 198. 201. 203. 203. 201. 210. 201. 199. 199. 202.\n",
    "  198. 201. 199. 201. 204. 196. 205. 198. 201. 201. 202.]\n",
    " [200. 202. 194. 208. 208. 198. 199. 202. 204. 198. 202. 201. 202. 198.\n",
    "  199. 198. 196. 199. 201. 202. 208. 203. 199. 198. 202.]\n",
    " [201. 201. 202. 199. 200. 217. 203. 199. 202. 201. 202. 201. 206. 201.\n",
    "  202. 207. 199. 199. 198. 200. 198. 200. 203. 200. 200.]\n",
    " [198. 199. 200. 198. 200. 201. 199. 206. 201. 196. 201. 200. 198. 201.\n",
    "  200. 201. 199. 203. 204. 199. 199. 201. 202. 209. 203.]]\n",
    "Label 1: [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "Measurement 1: [[199. 200. 200. 203. 204. 202. 199. 200. 197. 206. 200. 200. 199. 201.\n",
    "  201. 198. 202. 212. 201. 201. 195. 199. 198. 203. 199.]\n",
    " [203. 200. 195. 209. 200. 198. 200. 200. 202. 198. 201. 200. 198. 199.\n",
    "  201. 196. 201. 201. 198. 197. 202. 200. 199. 199. 193.]\n",
    " [203. 201. 201. 202. 198. 210. 200. 200. 201. 202. 200. 200. 201. 201.\n",
    "  198. 202. 209. 199. 200. 198. 201. 203. 200. 200. 206.]\n",
    " [201. 201. 200. 207. 201. 204. 203. 198. 201. 200. 199. 200. 198. 197.\n",
    "  200. 199. 199. 198. 202. 203. 198. 199. 202. 206. 200.]]\n",
    "Label 2: [[0.]\n",
    "...\n",
    " [199. 199. 203. 199. 196. 208. 200. 206. 200. 199. 201. 198. 200. 202.\n",
    "  198. 201. 198. 199. 201. 200. 198. 201. 200. 201. 198.]\n",
    " [199. 201. 203. 198. 200. 201. 202. 203. 202. 207. 201. 198. 196. 201.\n",
    "  195. 201. 201. 198. 198. 202. 197. 199. 202. 202. 200.]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syde_522",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
